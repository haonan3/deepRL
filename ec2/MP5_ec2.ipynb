{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_model import DQN_RAM\n",
    "from dqn_learn import OptimizerSpec, dqn_learing\n",
    "from utils.gym import get_ram_env, get_wrapper_by_name\n",
    "from utils.schedule import LinearSchedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:25:37,216] Making new env: Pong-ram-v0\n",
      "/home/dl/downloads/venv3/lib/python3.7/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2018-12-20 08:25:37,619] Clearing 10 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_ram_env(env, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE=1000000\n",
    "LEARNING_STARTS=50000\n",
    "LEARNING_FREQ=4\n",
    "FRAME_HISTORY_LEN=1\n",
    "TARGER_UPDATE_FREQ=10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_schedule = LinearSchedule(1000000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criterion(env):\n",
    "    # notice that here t is the number of steps of the wrapped env,\n",
    "    # which is different from the number of steps in the underlying env\n",
    "    num_timesteps=int(4e7)\n",
    "    return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:25:48,350] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000000.mp4\n",
      "[2018-12-20 08:25:49,836] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000001.mp4\n",
      "[2018-12-20 08:25:54,878] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000008.mp4\n",
      "[2018-12-20 08:26:06,486] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000027.mp4\n",
      "[2018-12-20 08:26:27,989] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000064.mp4\n",
      "[2018-12-20 08:27:03,171] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000125.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 60000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 202\n",
      "exploration 0.946000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:28:04,478] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000216.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 70000\n",
      "mean reward (100 episodes) -20.660000\n",
      "best mean reward -20.460000\n",
      "episodes 238\n",
      "exploration 0.937000\n",
      "Saved to statistics.pkl\n",
      "Timestep 80000\n",
      "mean reward (100 episodes) -20.580000\n",
      "best mean reward -20.460000\n",
      "episodes 272\n",
      "exploration 0.928000\n",
      "Saved to statistics.pkl\n",
      "Timestep 90000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 306\n",
      "exploration 0.919000\n",
      "Saved to statistics.pkl\n",
      "Timestep 100000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 340\n",
      "exploration 0.910000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:29:44,627] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000343.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 110000\n",
      "mean reward (100 episodes) -20.650000\n",
      "best mean reward -20.460000\n",
      "episodes 374\n",
      "exploration 0.901000\n",
      "Saved to statistics.pkl\n",
      "Timestep 120000\n",
      "mean reward (100 episodes) -20.740000\n",
      "best mean reward -20.460000\n",
      "episodes 409\n",
      "exploration 0.892000\n",
      "Saved to statistics.pkl\n",
      "Timestep 130000\n",
      "mean reward (100 episodes) -20.740000\n",
      "best mean reward -20.460000\n",
      "episodes 443\n",
      "exploration 0.883000\n",
      "Saved to statistics.pkl\n",
      "Timestep 140000\n",
      "mean reward (100 episodes) -20.670000\n",
      "best mean reward -20.460000\n",
      "episodes 477\n",
      "exploration 0.874000\n",
      "Saved to statistics.pkl\n",
      "Timestep 150000\n",
      "mean reward (100 episodes) -20.580000\n",
      "best mean reward -20.460000\n",
      "episodes 510\n",
      "exploration 0.865000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:32:02,323] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000512.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 160000\n",
      "mean reward (100 episodes) -20.520000\n",
      "best mean reward -20.460000\n",
      "episodes 544\n",
      "exploration 0.856000\n",
      "Saved to statistics.pkl\n",
      "Timestep 170000\n",
      "mean reward (100 episodes) -20.530000\n",
      "best mean reward -20.460000\n",
      "episodes 577\n",
      "exploration 0.847000\n",
      "Saved to statistics.pkl\n",
      "Timestep 180000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 610\n",
      "exploration 0.838000\n",
      "Saved to statistics.pkl\n",
      "Timestep 190000\n",
      "mean reward (100 episodes) -20.600000\n",
      "best mean reward -20.460000\n",
      "episodes 643\n",
      "exploration 0.829000\n",
      "Saved to statistics.pkl\n",
      "Timestep 200000\n",
      "mean reward (100 episodes) -20.540000\n",
      "best mean reward -20.460000\n",
      "episodes 677\n",
      "exploration 0.820000\n",
      "Saved to statistics.pkl\n",
      "Timestep 210000\n",
      "mean reward (100 episodes) -20.460000\n",
      "best mean reward -20.460000\n",
      "episodes 709\n",
      "exploration 0.811000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:35:05,091] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video000729.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 220000\n",
      "mean reward (100 episodes) -20.430000\n",
      "best mean reward -20.430000\n",
      "episodes 742\n",
      "exploration 0.802000\n",
      "Saved to statistics.pkl\n",
      "Timestep 230000\n",
      "mean reward (100 episodes) -20.440000\n",
      "best mean reward -20.430000\n",
      "episodes 776\n",
      "exploration 0.793000\n",
      "Saved to statistics.pkl\n",
      "Timestep 240000\n",
      "mean reward (100 episodes) -20.480000\n",
      "best mean reward -20.430000\n",
      "episodes 809\n",
      "exploration 0.784000\n",
      "Saved to statistics.pkl\n",
      "Timestep 250000\n",
      "mean reward (100 episodes) -20.520000\n",
      "best mean reward -20.430000\n",
      "episodes 841\n",
      "exploration 0.775000\n",
      "Saved to statistics.pkl\n",
      "Timestep 260000\n",
      "mean reward (100 episodes) -20.530000\n",
      "best mean reward -20.430000\n",
      "episodes 875\n",
      "exploration 0.766000\n",
      "Saved to statistics.pkl\n",
      "Timestep 270000\n",
      "mean reward (100 episodes) -20.510000\n",
      "best mean reward -20.430000\n",
      "episodes 908\n",
      "exploration 0.757000\n",
      "Saved to statistics.pkl\n",
      "Timestep 280000\n",
      "mean reward (100 episodes) -20.480000\n",
      "best mean reward -20.430000\n",
      "episodes 942\n",
      "exploration 0.748000\n",
      "Saved to statistics.pkl\n",
      "Timestep 290000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.390000\n",
      "episodes 974\n",
      "exploration 0.739000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:38:56,635] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video001000.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 300000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.380000\n",
      "episodes 1008\n",
      "exploration 0.730000\n",
      "Saved to statistics.pkl\n",
      "Timestep 310000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.360000\n",
      "episodes 1041\n",
      "exploration 0.721000\n",
      "Saved to statistics.pkl\n",
      "Timestep 320000\n",
      "mean reward (100 episodes) -20.500000\n",
      "best mean reward -20.360000\n",
      "episodes 1073\n",
      "exploration 0.712000\n",
      "Saved to statistics.pkl\n",
      "Timestep 330000\n",
      "mean reward (100 episodes) -20.460000\n",
      "best mean reward -20.360000\n",
      "episodes 1105\n",
      "exploration 0.703000\n",
      "Saved to statistics.pkl\n",
      "Timestep 340000\n",
      "mean reward (100 episodes) -20.420000\n",
      "best mean reward -20.360000\n",
      "episodes 1135\n",
      "exploration 0.694000\n",
      "Saved to statistics.pkl\n",
      "Timestep 350000\n",
      "mean reward (100 episodes) -20.410000\n",
      "best mean reward -20.360000\n",
      "episodes 1168\n",
      "exploration 0.685000\n",
      "Saved to statistics.pkl\n",
      "Timestep 360000\n",
      "mean reward (100 episodes) -20.370000\n",
      "best mean reward -20.320000\n",
      "episodes 1198\n",
      "exploration 0.676000\n",
      "Saved to statistics.pkl\n",
      "Timestep 370000\n",
      "mean reward (100 episodes) -20.390000\n",
      "best mean reward -20.320000\n",
      "episodes 1229\n",
      "exploration 0.667000\n",
      "Saved to statistics.pkl\n",
      "Timestep 380000\n",
      "mean reward (100 episodes) -20.290000\n",
      "best mean reward -20.280000\n",
      "episodes 1260\n",
      "exploration 0.658000\n",
      "Saved to statistics.pkl\n",
      "Timestep 390000\n",
      "mean reward (100 episodes) -20.280000\n",
      "best mean reward -20.270000\n",
      "episodes 1291\n",
      "exploration 0.649000\n",
      "Saved to statistics.pkl\n",
      "Timestep 400000\n",
      "mean reward (100 episodes) -20.150000\n",
      "best mean reward -20.150000\n",
      "episodes 1321\n",
      "exploration 0.640000\n",
      "Saved to statistics.pkl\n",
      "Timestep 410000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.130000\n",
      "episodes 1353\n",
      "exploration 0.631000\n",
      "Saved to statistics.pkl\n",
      "Timestep 420000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.130000\n",
      "episodes 1383\n",
      "exploration 0.622000\n",
      "Saved to statistics.pkl\n",
      "Timestep 430000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.130000\n",
      "episodes 1414\n",
      "exploration 0.613000\n",
      "Saved to statistics.pkl\n",
      "Timestep 440000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.130000\n",
      "episodes 1443\n",
      "exploration 0.604000\n",
      "Saved to statistics.pkl\n",
      "Timestep 450000\n",
      "mean reward (100 episodes) -20.290000\n",
      "best mean reward -20.130000\n",
      "episodes 1474\n",
      "exploration 0.595000\n",
      "Saved to statistics.pkl\n",
      "Timestep 460000\n",
      "mean reward (100 episodes) -20.320000\n",
      "best mean reward -20.130000\n",
      "episodes 1504\n",
      "exploration 0.586000\n",
      "Saved to statistics.pkl\n",
      "Timestep 470000\n",
      "mean reward (100 episodes) -20.260000\n",
      "best mean reward -20.130000\n",
      "episodes 1533\n",
      "exploration 0.577000\n",
      "Saved to statistics.pkl\n",
      "Timestep 480000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.130000\n",
      "episodes 1562\n",
      "exploration 0.568000\n",
      "Saved to statistics.pkl\n",
      "Timestep 490000\n",
      "mean reward (100 episodes) -20.140000\n",
      "best mean reward -20.100000\n",
      "episodes 1592\n",
      "exploration 0.559000\n",
      "Saved to statistics.pkl\n",
      "Timestep 500000\n",
      "mean reward (100 episodes) -20.090000\n",
      "best mean reward -20.060000\n",
      "episodes 1622\n",
      "exploration 0.550000\n",
      "Saved to statistics.pkl\n",
      "Timestep 510000\n",
      "mean reward (100 episodes) -20.300000\n",
      "best mean reward -20.060000\n",
      "episodes 1654\n",
      "exploration 0.541000\n",
      "Saved to statistics.pkl\n",
      "Timestep 520000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.060000\n",
      "episodes 1686\n",
      "exploration 0.532000\n",
      "Saved to statistics.pkl\n",
      "Timestep 530000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.060000\n",
      "episodes 1717\n",
      "exploration 0.523000\n",
      "Saved to statistics.pkl\n",
      "Timestep 540000\n",
      "mean reward (100 episodes) -20.060000\n",
      "best mean reward -20.060000\n",
      "episodes 1744\n",
      "exploration 0.514000\n",
      "Saved to statistics.pkl\n",
      "Timestep 550000\n",
      "mean reward (100 episodes) -20.030000\n",
      "best mean reward -20.000000\n",
      "episodes 1773\n",
      "exploration 0.505000\n",
      "Saved to statistics.pkl\n",
      "Timestep 560000\n",
      "mean reward (100 episodes) -20.020000\n",
      "best mean reward -20.000000\n",
      "episodes 1803\n",
      "exploration 0.496000\n",
      "Saved to statistics.pkl\n",
      "Timestep 570000\n",
      "mean reward (100 episodes) -20.170000\n",
      "best mean reward -20.000000\n",
      "episodes 1832\n",
      "exploration 0.487000\n",
      "Saved to statistics.pkl\n",
      "Timestep 580000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 1863\n",
      "exploration 0.478000\n",
      "Saved to statistics.pkl\n",
      "Timestep 590000\n",
      "mean reward (100 episodes) -20.280000\n",
      "best mean reward -20.000000\n",
      "episodes 1894\n",
      "exploration 0.469000\n",
      "Saved to statistics.pkl\n",
      "Timestep 600000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.000000\n",
      "episodes 1924\n",
      "exploration 0.460000\n",
      "Saved to statistics.pkl\n",
      "Timestep 610000\n",
      "mean reward (100 episodes) -20.220000\n",
      "best mean reward -20.000000\n",
      "episodes 1955\n",
      "exploration 0.451000\n",
      "Saved to statistics.pkl\n",
      "Timestep 620000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.000000\n",
      "episodes 1985\n",
      "exploration 0.442000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 08:55:23,815] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video002000.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 630000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.000000\n",
      "episodes 2014\n",
      "exploration 0.433000\n",
      "Saved to statistics.pkl\n",
      "Timestep 640000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 2045\n",
      "exploration 0.424000\n",
      "Saved to statistics.pkl\n",
      "Timestep 650000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.000000\n",
      "episodes 2073\n",
      "exploration 0.415000\n",
      "Saved to statistics.pkl\n",
      "Timestep 660000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.000000\n",
      "episodes 2102\n",
      "exploration 0.406000\n",
      "Saved to statistics.pkl\n",
      "Timestep 670000\n",
      "mean reward (100 episodes) -20.310000\n",
      "best mean reward -20.000000\n",
      "episodes 2132\n",
      "exploration 0.397000\n",
      "Saved to statistics.pkl\n",
      "Timestep 680000\n",
      "mean reward (100 episodes) -20.320000\n",
      "best mean reward -20.000000\n",
      "episodes 2160\n",
      "exploration 0.388000\n",
      "Saved to statistics.pkl\n",
      "Timestep 690000\n",
      "mean reward (100 episodes) -20.420000\n",
      "best mean reward -20.000000\n",
      "episodes 2189\n",
      "exploration 0.379000\n",
      "Saved to statistics.pkl\n",
      "Timestep 700000\n",
      "mean reward (100 episodes) -20.380000\n",
      "best mean reward -20.000000\n",
      "episodes 2220\n",
      "exploration 0.370000\n",
      "Saved to statistics.pkl\n",
      "Timestep 710000\n",
      "mean reward (100 episodes) -20.310000\n",
      "best mean reward -20.000000\n",
      "episodes 2249\n",
      "exploration 0.361000\n",
      "Saved to statistics.pkl\n",
      "Timestep 720000\n",
      "mean reward (100 episodes) -20.280000\n",
      "best mean reward -20.000000\n",
      "episodes 2277\n",
      "exploration 0.352000\n",
      "Saved to statistics.pkl\n",
      "Timestep 730000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 2305\n",
      "exploration 0.343000\n",
      "Saved to statistics.pkl\n",
      "Timestep 740000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.000000\n",
      "episodes 2335\n",
      "exploration 0.334000\n",
      "Saved to statistics.pkl\n",
      "Timestep 750000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.000000\n",
      "episodes 2362\n",
      "exploration 0.325000\n",
      "Saved to statistics.pkl\n",
      "Timestep 760000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.000000\n",
      "episodes 2389\n",
      "exploration 0.316000\n",
      "Saved to statistics.pkl\n",
      "Timestep 770000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.000000\n",
      "episodes 2415\n",
      "exploration 0.307000\n",
      "Saved to statistics.pkl\n",
      "Timestep 780000\n",
      "mean reward (100 episodes) -20.110000\n",
      "best mean reward -20.000000\n",
      "episodes 2443\n",
      "exploration 0.298000\n",
      "Saved to statistics.pkl\n",
      "Timestep 790000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.000000\n",
      "episodes 2473\n",
      "exploration 0.289000\n",
      "Saved to statistics.pkl\n",
      "Timestep 800000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.000000\n",
      "episodes 2501\n",
      "exploration 0.280000\n",
      "Saved to statistics.pkl\n",
      "Timestep 810000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 2528\n",
      "exploration 0.271000\n",
      "Saved to statistics.pkl\n",
      "Timestep 820000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.000000\n",
      "episodes 2554\n",
      "exploration 0.262000\n",
      "Saved to statistics.pkl\n",
      "Timestep 830000\n",
      "mean reward (100 episodes) -20.080000\n",
      "best mean reward -20.000000\n",
      "episodes 2581\n",
      "exploration 0.253000\n",
      "Saved to statistics.pkl\n",
      "Timestep 840000\n",
      "mean reward (100 episodes) -20.010000\n",
      "best mean reward -20.000000\n",
      "episodes 2607\n",
      "exploration 0.244000\n",
      "Saved to statistics.pkl\n",
      "Timestep 850000\n",
      "mean reward (100 episodes) -19.910000\n",
      "best mean reward -19.910000\n",
      "episodes 2632\n",
      "exploration 0.235000\n",
      "Saved to statistics.pkl\n",
      "Timestep 860000\n",
      "mean reward (100 episodes) -19.870000\n",
      "best mean reward -19.870000\n",
      "episodes 2659\n",
      "exploration 0.226000\n",
      "Saved to statistics.pkl\n",
      "Timestep 870000\n",
      "mean reward (100 episodes) -19.750000\n",
      "best mean reward -19.750000\n",
      "episodes 2684\n",
      "exploration 0.217000\n",
      "Saved to statistics.pkl\n",
      "Timestep 880000\n",
      "mean reward (100 episodes) -19.700000\n",
      "best mean reward -19.680000\n",
      "episodes 2709\n",
      "exploration 0.208000\n",
      "Saved to statistics.pkl\n",
      "Timestep 890000\n",
      "mean reward (100 episodes) -19.770000\n",
      "best mean reward -19.680000\n",
      "episodes 2735\n",
      "exploration 0.199000\n",
      "Saved to statistics.pkl\n",
      "Timestep 900000\n",
      "mean reward (100 episodes) -19.630000\n",
      "best mean reward -19.630000\n",
      "episodes 2760\n",
      "exploration 0.190000\n",
      "Saved to statistics.pkl\n",
      "Timestep 910000\n",
      "mean reward (100 episodes) -19.770000\n",
      "best mean reward -19.630000\n",
      "episodes 2786\n",
      "exploration 0.181000\n",
      "Saved to statistics.pkl\n",
      "Timestep 920000\n",
      "mean reward (100 episodes) -19.830000\n",
      "best mean reward -19.630000\n",
      "episodes 2812\n",
      "exploration 0.172000\n",
      "Saved to statistics.pkl\n",
      "Timestep 930000\n",
      "mean reward (100 episodes) -19.920000\n",
      "best mean reward -19.630000\n",
      "episodes 2839\n",
      "exploration 0.163000\n",
      "Saved to statistics.pkl\n",
      "Timestep 940000\n",
      "mean reward (100 episodes) -19.920000\n",
      "best mean reward -19.630000\n",
      "episodes 2863\n",
      "exploration 0.154000\n",
      "Saved to statistics.pkl\n",
      "Timestep 950000\n",
      "mean reward (100 episodes) -19.740000\n",
      "best mean reward -19.630000\n",
      "episodes 2888\n",
      "exploration 0.145000\n",
      "Saved to statistics.pkl\n",
      "Timestep 960000\n",
      "mean reward (100 episodes) -19.790000\n",
      "best mean reward -19.630000\n",
      "episodes 2913\n",
      "exploration 0.136000\n",
      "Saved to statistics.pkl\n",
      "Timestep 970000\n",
      "mean reward (100 episodes) -19.670000\n",
      "best mean reward -19.630000\n",
      "episodes 2937\n",
      "exploration 0.127000\n",
      "Saved to statistics.pkl\n",
      "Timestep 980000\n",
      "mean reward (100 episodes) -19.770000\n",
      "best mean reward -19.620000\n",
      "episodes 2963\n",
      "exploration 0.118000\n",
      "Saved to statistics.pkl\n",
      "Timestep 990000\n",
      "mean reward (100 episodes) -19.850000\n",
      "best mean reward -19.620000\n",
      "episodes 2988\n",
      "exploration 0.109000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 09:15:33,012] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.31218.video003000.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1000000\n",
      "mean reward (100 episodes) -19.860000\n",
      "best mean reward -19.620000\n",
      "episodes 3015\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1010000\n",
      "mean reward (100 episodes) -19.920000\n",
      "best mean reward -19.620000\n",
      "episodes 3039\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1020000\n",
      "mean reward (100 episodes) -19.840000\n",
      "best mean reward -19.620000\n",
      "episodes 3064\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1030000\n",
      "mean reward (100 episodes) -19.900000\n",
      "best mean reward -19.620000\n",
      "episodes 3090\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1040000\n",
      "mean reward (100 episodes) -19.890000\n",
      "best mean reward -19.620000\n",
      "episodes 3116\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1050000\n",
      "mean reward (100 episodes) -19.820000\n",
      "best mean reward -19.620000\n",
      "episodes 3141\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1060000\n",
      "mean reward (100 episodes) -19.790000\n",
      "best mean reward -19.620000\n",
      "episodes 3164\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1070000\n",
      "mean reward (100 episodes) -19.690000\n",
      "best mean reward -19.620000\n",
      "episodes 3186\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1080000\n",
      "mean reward (100 episodes) -19.610000\n",
      "best mean reward -19.560000\n",
      "episodes 3210\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1090000\n",
      "mean reward (100 episodes) -19.450000\n",
      "best mean reward -19.450000\n",
      "episodes 3232\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1100000\n",
      "mean reward (100 episodes) -19.510000\n",
      "best mean reward -19.420000\n",
      "episodes 3257\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1110000\n",
      "mean reward (100 episodes) -19.520000\n",
      "best mean reward -19.420000\n",
      "episodes 3280\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1120000\n",
      "mean reward (100 episodes) -19.490000\n",
      "best mean reward -19.420000\n",
      "episodes 3305\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1130000\n",
      "mean reward (100 episodes) -19.670000\n",
      "best mean reward -19.420000\n",
      "episodes 3330\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1140000\n",
      "mean reward (100 episodes) -19.580000\n",
      "best mean reward -19.420000\n",
      "episodes 3353\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n",
      "Timestep 1150000\n",
      "mean reward (100 episodes) -19.340000\n",
      "best mean reward -19.340000\n",
      "episodes 3376\n",
      "exploration 0.100000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2c172cc9deed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlearning_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mframe_history_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFRAME_HISTORY_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtarget_update_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGER_UPDATE_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m~/downloads/github/deepRL/ec2/dqn_learn.py\u001b[0m in \u001b[0;36mdqn_learing\u001b[0;34m(env, q_func, optimizer_spec, exploration, stopping_criterion, replay_buffer_size, batch_size, gamma, learning_starts, learning_freq, frame_history_len, target_update_freq)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mact_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mrew_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mnext_obs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mnot_done_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN_RAM,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGER_UPDATE_FREQ,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
