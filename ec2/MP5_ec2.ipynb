{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_model import DQN_RAM\n",
    "from dqn_learn import OptimizerSpec, dqn_learing\n",
    "from utils.gym import get_ram_env, get_wrapper_by_name\n",
    "from utils.schedule import LinearSchedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 06:56:59,432] Making new env: Pong-ram-v0\n",
      "/home/dl/downloads/venv3/lib/python3.7/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "[2018-12-20 06:56:59,848] Clearing 16 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "# Run training\n",
    "seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "env = get_ram_env(env, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE=1000000\n",
    "LEARNING_STARTS=50000\n",
    "LEARNING_FREQ=4\n",
    "FRAME_HISTORY_LEN=1\n",
    "TARGER_UPDATE_FREQ=10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_schedule = LinearSchedule(1000000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criterion(env):\n",
    "    # notice that here t is the number of steps of the wrapped env,\n",
    "    # which is different from the number of steps in the underlying env\n",
    "    num_timesteps=int(4e7)\n",
    "    return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 06:57:02,700] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000000.mp4\n",
      "[2018-12-20 06:57:04,131] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000001.mp4\n",
      "[2018-12-20 06:57:09,014] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000008.mp4\n",
      "[2018-12-20 06:57:20,203] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000027.mp4\n",
      "[2018-12-20 06:57:41,406] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000064.mp4\n",
      "[2018-12-20 06:58:15,827] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000125.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 60000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 202\n",
      "exploration 0.946000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 06:59:16,301] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000216.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 70000\n",
      "mean reward (100 episodes) -20.660000\n",
      "best mean reward -20.460000\n",
      "episodes 238\n",
      "exploration 0.937000\n",
      "Saved to statistics.pkl\n",
      "Timestep 80000\n",
      "mean reward (100 episodes) -20.580000\n",
      "best mean reward -20.460000\n",
      "episodes 272\n",
      "exploration 0.928000\n",
      "Saved to statistics.pkl\n",
      "Timestep 90000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 306\n",
      "exploration 0.919000\n",
      "Saved to statistics.pkl\n",
      "Timestep 100000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 340\n",
      "exploration 0.910000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 07:00:53,589] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000343.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 110000\n",
      "mean reward (100 episodes) -20.650000\n",
      "best mean reward -20.460000\n",
      "episodes 374\n",
      "exploration 0.901000\n",
      "Saved to statistics.pkl\n",
      "Timestep 120000\n",
      "mean reward (100 episodes) -20.740000\n",
      "best mean reward -20.460000\n",
      "episodes 409\n",
      "exploration 0.892000\n",
      "Saved to statistics.pkl\n",
      "Timestep 130000\n",
      "mean reward (100 episodes) -20.740000\n",
      "best mean reward -20.460000\n",
      "episodes 443\n",
      "exploration 0.883000\n",
      "Saved to statistics.pkl\n",
      "Timestep 140000\n",
      "mean reward (100 episodes) -20.670000\n",
      "best mean reward -20.460000\n",
      "episodes 477\n",
      "exploration 0.874000\n",
      "Saved to statistics.pkl\n",
      "Timestep 150000\n",
      "mean reward (100 episodes) -20.580000\n",
      "best mean reward -20.460000\n",
      "episodes 510\n",
      "exploration 0.865000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 07:03:07,858] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000512.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 160000\n",
      "mean reward (100 episodes) -20.520000\n",
      "best mean reward -20.460000\n",
      "episodes 544\n",
      "exploration 0.856000\n",
      "Saved to statistics.pkl\n",
      "Timestep 170000\n",
      "mean reward (100 episodes) -20.530000\n",
      "best mean reward -20.460000\n",
      "episodes 577\n",
      "exploration 0.847000\n",
      "Saved to statistics.pkl\n",
      "Timestep 180000\n",
      "mean reward (100 episodes) -20.590000\n",
      "best mean reward -20.460000\n",
      "episodes 610\n",
      "exploration 0.838000\n",
      "Saved to statistics.pkl\n",
      "Timestep 190000\n",
      "mean reward (100 episodes) -20.600000\n",
      "best mean reward -20.460000\n",
      "episodes 643\n",
      "exploration 0.829000\n",
      "Saved to statistics.pkl\n",
      "Timestep 200000\n",
      "mean reward (100 episodes) -20.540000\n",
      "best mean reward -20.460000\n",
      "episodes 677\n",
      "exploration 0.820000\n",
      "Saved to statistics.pkl\n",
      "Timestep 210000\n",
      "mean reward (100 episodes) -20.460000\n",
      "best mean reward -20.460000\n",
      "episodes 709\n",
      "exploration 0.811000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 07:06:06,481] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video000729.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 220000\n",
      "mean reward (100 episodes) -20.430000\n",
      "best mean reward -20.430000\n",
      "episodes 742\n",
      "exploration 0.802000\n",
      "Saved to statistics.pkl\n",
      "Timestep 230000\n",
      "mean reward (100 episodes) -20.440000\n",
      "best mean reward -20.430000\n",
      "episodes 776\n",
      "exploration 0.793000\n",
      "Saved to statistics.pkl\n",
      "Timestep 240000\n",
      "mean reward (100 episodes) -20.480000\n",
      "best mean reward -20.430000\n",
      "episodes 809\n",
      "exploration 0.784000\n",
      "Saved to statistics.pkl\n",
      "Timestep 250000\n",
      "mean reward (100 episodes) -20.520000\n",
      "best mean reward -20.430000\n",
      "episodes 841\n",
      "exploration 0.775000\n",
      "Saved to statistics.pkl\n",
      "Timestep 260000\n",
      "mean reward (100 episodes) -20.530000\n",
      "best mean reward -20.430000\n",
      "episodes 875\n",
      "exploration 0.766000\n",
      "Saved to statistics.pkl\n",
      "Timestep 270000\n",
      "mean reward (100 episodes) -20.510000\n",
      "best mean reward -20.430000\n",
      "episodes 908\n",
      "exploration 0.757000\n",
      "Saved to statistics.pkl\n",
      "Timestep 280000\n",
      "mean reward (100 episodes) -20.480000\n",
      "best mean reward -20.430000\n",
      "episodes 942\n",
      "exploration 0.748000\n",
      "Saved to statistics.pkl\n",
      "Timestep 290000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.390000\n",
      "episodes 974\n",
      "exploration 0.739000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 07:09:53,169] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video001000.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 300000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.380000\n",
      "episodes 1008\n",
      "exploration 0.730000\n",
      "Saved to statistics.pkl\n",
      "Timestep 310000\n",
      "mean reward (100 episodes) -20.400000\n",
      "best mean reward -20.360000\n",
      "episodes 1041\n",
      "exploration 0.721000\n",
      "Saved to statistics.pkl\n",
      "Timestep 320000\n",
      "mean reward (100 episodes) -20.500000\n",
      "best mean reward -20.360000\n",
      "episodes 1073\n",
      "exploration 0.712000\n",
      "Saved to statistics.pkl\n",
      "Timestep 330000\n",
      "mean reward (100 episodes) -20.460000\n",
      "best mean reward -20.360000\n",
      "episodes 1105\n",
      "exploration 0.703000\n",
      "Saved to statistics.pkl\n",
      "Timestep 340000\n",
      "mean reward (100 episodes) -20.420000\n",
      "best mean reward -20.360000\n",
      "episodes 1135\n",
      "exploration 0.694000\n",
      "Saved to statistics.pkl\n",
      "Timestep 350000\n",
      "mean reward (100 episodes) -20.410000\n",
      "best mean reward -20.360000\n",
      "episodes 1168\n",
      "exploration 0.685000\n",
      "Saved to statistics.pkl\n",
      "Timestep 360000\n",
      "mean reward (100 episodes) -20.370000\n",
      "best mean reward -20.320000\n",
      "episodes 1198\n",
      "exploration 0.676000\n",
      "Saved to statistics.pkl\n",
      "Timestep 370000\n",
      "mean reward (100 episodes) -20.390000\n",
      "best mean reward -20.320000\n",
      "episodes 1229\n",
      "exploration 0.667000\n",
      "Saved to statistics.pkl\n",
      "Timestep 380000\n",
      "mean reward (100 episodes) -20.290000\n",
      "best mean reward -20.280000\n",
      "episodes 1260\n",
      "exploration 0.658000\n",
      "Saved to statistics.pkl\n",
      "Timestep 390000\n",
      "mean reward (100 episodes) -20.280000\n",
      "best mean reward -20.270000\n",
      "episodes 1291\n",
      "exploration 0.649000\n",
      "Saved to statistics.pkl\n",
      "Timestep 400000\n",
      "mean reward (100 episodes) -20.150000\n",
      "best mean reward -20.150000\n",
      "episodes 1321\n",
      "exploration 0.640000\n",
      "Saved to statistics.pkl\n",
      "Timestep 410000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.130000\n",
      "episodes 1353\n",
      "exploration 0.631000\n",
      "Saved to statistics.pkl\n",
      "Timestep 420000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.130000\n",
      "episodes 1383\n",
      "exploration 0.622000\n",
      "Saved to statistics.pkl\n",
      "Timestep 430000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.130000\n",
      "episodes 1414\n",
      "exploration 0.613000\n",
      "Saved to statistics.pkl\n",
      "Timestep 440000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.130000\n",
      "episodes 1443\n",
      "exploration 0.604000\n",
      "Saved to statistics.pkl\n",
      "Timestep 450000\n",
      "mean reward (100 episodes) -20.290000\n",
      "best mean reward -20.130000\n",
      "episodes 1474\n",
      "exploration 0.595000\n",
      "Saved to statistics.pkl\n",
      "Timestep 460000\n",
      "mean reward (100 episodes) -20.320000\n",
      "best mean reward -20.130000\n",
      "episodes 1504\n",
      "exploration 0.586000\n",
      "Saved to statistics.pkl\n",
      "Timestep 470000\n",
      "mean reward (100 episodes) -20.260000\n",
      "best mean reward -20.130000\n",
      "episodes 1533\n",
      "exploration 0.577000\n",
      "Saved to statistics.pkl\n",
      "Timestep 480000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.130000\n",
      "episodes 1562\n",
      "exploration 0.568000\n",
      "Saved to statistics.pkl\n",
      "Timestep 490000\n",
      "mean reward (100 episodes) -20.140000\n",
      "best mean reward -20.100000\n",
      "episodes 1592\n",
      "exploration 0.559000\n",
      "Saved to statistics.pkl\n",
      "Timestep 500000\n",
      "mean reward (100 episodes) -20.090000\n",
      "best mean reward -20.060000\n",
      "episodes 1622\n",
      "exploration 0.550000\n",
      "Saved to statistics.pkl\n",
      "Timestep 510000\n",
      "mean reward (100 episodes) -20.300000\n",
      "best mean reward -20.060000\n",
      "episodes 1654\n",
      "exploration 0.541000\n",
      "Saved to statistics.pkl\n",
      "Timestep 520000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.060000\n",
      "episodes 1686\n",
      "exploration 0.532000\n",
      "Saved to statistics.pkl\n",
      "Timestep 530000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.060000\n",
      "episodes 1717\n",
      "exploration 0.523000\n",
      "Saved to statistics.pkl\n",
      "Timestep 540000\n",
      "mean reward (100 episodes) -20.060000\n",
      "best mean reward -20.060000\n",
      "episodes 1744\n",
      "exploration 0.514000\n",
      "Saved to statistics.pkl\n",
      "Timestep 550000\n",
      "mean reward (100 episodes) -20.030000\n",
      "best mean reward -20.000000\n",
      "episodes 1773\n",
      "exploration 0.505000\n",
      "Saved to statistics.pkl\n",
      "Timestep 560000\n",
      "mean reward (100 episodes) -20.020000\n",
      "best mean reward -20.000000\n",
      "episodes 1803\n",
      "exploration 0.496000\n",
      "Saved to statistics.pkl\n",
      "Timestep 570000\n",
      "mean reward (100 episodes) -20.170000\n",
      "best mean reward -20.000000\n",
      "episodes 1832\n",
      "exploration 0.487000\n",
      "Saved to statistics.pkl\n",
      "Timestep 580000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 1863\n",
      "exploration 0.478000\n",
      "Saved to statistics.pkl\n",
      "Timestep 590000\n",
      "mean reward (100 episodes) -20.280000\n",
      "best mean reward -20.000000\n",
      "episodes 1894\n",
      "exploration 0.469000\n",
      "Saved to statistics.pkl\n",
      "Timestep 600000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.000000\n",
      "episodes 1924\n",
      "exploration 0.460000\n",
      "Saved to statistics.pkl\n",
      "Timestep 610000\n",
      "mean reward (100 episodes) -20.220000\n",
      "best mean reward -20.000000\n",
      "episodes 1955\n",
      "exploration 0.451000\n",
      "Saved to statistics.pkl\n",
      "Timestep 620000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.000000\n",
      "episodes 1985\n",
      "exploration 0.442000\n",
      "Saved to statistics.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-12-20 07:26:01,412] Starting new video recorder writing to /tmp/gym-results/openaigym.video.0.29608.video002000.mp4\n",
      "/home/dl/downloads/github/deepRL/ec2/dqn_learn.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return model(Variable(obs, volatile=True)).data.max(1)[1].view(1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 630000\n",
      "mean reward (100 episodes) -20.210000\n",
      "best mean reward -20.000000\n",
      "episodes 2014\n",
      "exploration 0.433000\n",
      "Saved to statistics.pkl\n",
      "Timestep 640000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.000000\n",
      "episodes 2045\n",
      "exploration 0.424000\n",
      "Saved to statistics.pkl\n",
      "Timestep 650000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.000000\n",
      "episodes 2073\n",
      "exploration 0.415000\n",
      "Saved to statistics.pkl\n",
      "Timestep 660000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.000000\n",
      "episodes 2102\n",
      "exploration 0.406000\n",
      "Saved to statistics.pkl\n",
      "Timestep 670000\n",
      "mean reward (100 episodes) -20.310000\n",
      "best mean reward -20.000000\n",
      "episodes 2132\n",
      "exploration 0.397000\n",
      "Saved to statistics.pkl\n"
     ]
    }
   ],
   "source": [
    "dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN_RAM,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGER_UPDATE_FREQ,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
