{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -19.0   memory length: 3863   epsilon: 1.0    steps: 3863     evaluation reward: -19.0\n",
      "episode: 1   score: -21.0   memory length: 8103   epsilon: 1.0    steps: 4240     evaluation reward: -20.0\n",
      "episode: 2   score: -21.0   memory length: 11879   epsilon: 0.9813879999999866    steps: 3776     evaluation reward: -20.333333333333332\n",
      "episode: 3   score: -20.0   memory length: 15544   epsilon: 0.9451044999999605    steps: 3665     evaluation reward: -20.25\n",
      "episode: 4   score: -21.0   memory length: 19314   epsilon: 0.9077814999999336    steps: 3770     evaluation reward: -20.4\n",
      "episode: 5   score: -21.0   memory length: 22370   epsilon: 0.8775270999999119    steps: 3056     evaluation reward: -20.5\n",
      "episode: 6   score: -20.0   memory length: 27065   epsilon: 0.8310465999998784    steps: 4695     evaluation reward: -20.428571428571427\n",
      "episode: 7   score: -21.0   memory length: 30605   epsilon: 0.7960005999998532    steps: 3540     evaluation reward: -20.5\n",
      "episode: 8   score: -21.0   memory length: 33905   epsilon: 0.7633305999998297    steps: 3300     evaluation reward: -20.555555555555557\n",
      "episode: 9   score: -20.0   memory length: 38464   epsilon: 0.7181964999997972    steps: 4559     evaluation reward: -20.5\n",
      "episode: 10   score: -21.0   memory length: 41635   epsilon: 0.6868035999997746    steps: 3171     evaluation reward: -20.545454545454547\n",
      "episode: 11   score: -20.0   memory length: 45970   epsilon: 0.6438870999997437    steps: 4335     evaluation reward: -20.5\n",
      "now time :  2018-12-20 01:18:28.426765\n",
      "episode: 12   score: -21.0   memory length: 50541   epsilon: 0.5986341999997111    steps: 4571     evaluation reward: -20.53846153846154\n",
      "episode: 13   score: -20.0   memory length: 54688   epsilon: 0.5575788999996816    steps: 4147     evaluation reward: -20.5\n",
      "episode: 14   score: -17.0   memory length: 60334   epsilon: 0.5016834999996413    steps: 5646     evaluation reward: -20.266666666666666\n",
      "episode: 15   score: -15.0   memory length: 69998   epsilon: 0.4060098999995725    steps: 9664     evaluation reward: -19.9375\n",
      "episode: 16   score: -19.0   memory length: 75758   epsilon: 0.34898589999953145    steps: 5760     evaluation reward: -19.88235294117647\n",
      "episode: 17   score: -13.0   memory length: 83576   epsilon: 0.27158769999947574    steps: 7818     evaluation reward: -19.5\n",
      "episode: 18   score: -17.0   memory length: 93878   epsilon: 0.16959789999940234    steps: 10302     evaluation reward: -19.36842105263158\n",
      "now time :  2018-12-20 01:30:02.710942\n",
      "episode: 19   score: -17.0   memory length: 102812   epsilon: 0.08115129999940016    steps: 8934     evaluation reward: -19.25\n",
      "episode: 20   score: -18.0   memory length: 111449   epsilon: 0.009999999999411882    steps: 8637     evaluation reward: -19.19047619047619\n",
      "episode: 21   score: -18.0   memory length: 120774   epsilon: 0.009999999999411882    steps: 9325     evaluation reward: -19.136363636363637\n",
      "episode: 22   score: -17.0   memory length: 131830   epsilon: 0.009999999999411882    steps: 11056     evaluation reward: -19.043478260869566\n",
      "episode: 23   score: -16.0   memory length: 142777   epsilon: 0.009999999999411882    steps: 10947     evaluation reward: -18.916666666666668\n",
      "episode: 24   score: -19.0   memory length: 148857   epsilon: 0.009999999999411882    steps: 6080     evaluation reward: -18.92\n",
      "now time :  2018-12-20 01:42:28.393113\n",
      "episode: 25   score: -18.0   memory length: 157247   epsilon: 0.009999999999411882    steps: 8390     evaluation reward: -18.884615384615383\n",
      "episode: 26   score: -18.0   memory length: 166676   epsilon: 0.009999999999411882    steps: 9429     evaluation reward: -18.85185185185185\n",
      "episode: 27   score: -15.0   memory length: 175211   epsilon: 0.009999999999411882    steps: 8535     evaluation reward: -18.714285714285715\n",
      "episode: 28   score: -13.0   memory length: 185459   epsilon: 0.009999999999411882    steps: 10248     evaluation reward: -18.517241379310345\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        #if render_breakout:\n",
    "        #    env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/ec1_breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > (-15):\n",
    "                torch.save(agent.policy_net, \"./save_model/ec1_pongv4\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/ec1_pongv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
