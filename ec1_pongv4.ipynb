{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl/.local/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -19.0   memory length: 3863   epsilon: 1.0    steps: 3863     evaluation reward: -19.0\n",
      "episode: 1   score: -21.0   memory length: 8103   epsilon: 1.0    steps: 4240     evaluation reward: -20.0\n",
      "episode: 2   score: -21.0   memory length: 11879   epsilon: 0.9813879999999866    steps: 3776     evaluation reward: -20.333333333333332\n",
      "episode: 3   score: -20.0   memory length: 15544   epsilon: 0.9451044999999605    steps: 3665     evaluation reward: -20.25\n",
      "episode: 4   score: -21.0   memory length: 19314   epsilon: 0.9077814999999336    steps: 3770     evaluation reward: -20.4\n",
      "episode: 5   score: -21.0   memory length: 22370   epsilon: 0.8775270999999119    steps: 3056     evaluation reward: -20.5\n",
      "episode: 6   score: -20.0   memory length: 27065   epsilon: 0.8310465999998784    steps: 4695     evaluation reward: -20.428571428571427\n",
      "episode: 7   score: -21.0   memory length: 30605   epsilon: 0.7960005999998532    steps: 3540     evaluation reward: -20.5\n",
      "episode: 8   score: -21.0   memory length: 33905   epsilon: 0.7633305999998297    steps: 3300     evaluation reward: -20.555555555555557\n",
      "episode: 9   score: -20.0   memory length: 38464   epsilon: 0.7181964999997972    steps: 4559     evaluation reward: -20.5\n",
      "episode: 10   score: -21.0   memory length: 41635   epsilon: 0.6868035999997746    steps: 3171     evaluation reward: -20.545454545454547\n",
      "episode: 11   score: -20.0   memory length: 45970   epsilon: 0.6438870999997437    steps: 4335     evaluation reward: -20.5\n",
      "now time :  2018-12-20 01:18:28.426765\n",
      "episode: 12   score: -21.0   memory length: 50541   epsilon: 0.5986341999997111    steps: 4571     evaluation reward: -20.53846153846154\n",
      "episode: 13   score: -20.0   memory length: 54688   epsilon: 0.5575788999996816    steps: 4147     evaluation reward: -20.5\n",
      "episode: 14   score: -17.0   memory length: 60334   epsilon: 0.5016834999996413    steps: 5646     evaluation reward: -20.266666666666666\n",
      "episode: 15   score: -15.0   memory length: 69998   epsilon: 0.4060098999995725    steps: 9664     evaluation reward: -19.9375\n",
      "episode: 16   score: -19.0   memory length: 75758   epsilon: 0.34898589999953145    steps: 5760     evaluation reward: -19.88235294117647\n",
      "episode: 17   score: -13.0   memory length: 83576   epsilon: 0.27158769999947574    steps: 7818     evaluation reward: -19.5\n",
      "episode: 18   score: -17.0   memory length: 93878   epsilon: 0.16959789999940234    steps: 10302     evaluation reward: -19.36842105263158\n",
      "now time :  2018-12-20 01:30:02.710942\n",
      "episode: 19   score: -17.0   memory length: 102812   epsilon: 0.08115129999940016    steps: 8934     evaluation reward: -19.25\n",
      "episode: 20   score: -18.0   memory length: 111449   epsilon: 0.009999999999411882    steps: 8637     evaluation reward: -19.19047619047619\n",
      "episode: 21   score: -18.0   memory length: 120774   epsilon: 0.009999999999411882    steps: 9325     evaluation reward: -19.136363636363637\n",
      "episode: 22   score: -17.0   memory length: 131830   epsilon: 0.009999999999411882    steps: 11056     evaluation reward: -19.043478260869566\n",
      "episode: 23   score: -16.0   memory length: 142777   epsilon: 0.009999999999411882    steps: 10947     evaluation reward: -18.916666666666668\n",
      "episode: 24   score: -19.0   memory length: 148857   epsilon: 0.009999999999411882    steps: 6080     evaluation reward: -18.92\n",
      "now time :  2018-12-20 01:42:28.393113\n",
      "episode: 25   score: -18.0   memory length: 157247   epsilon: 0.009999999999411882    steps: 8390     evaluation reward: -18.884615384615383\n",
      "episode: 26   score: -18.0   memory length: 166676   epsilon: 0.009999999999411882    steps: 9429     evaluation reward: -18.85185185185185\n",
      "episode: 27   score: -15.0   memory length: 175211   epsilon: 0.009999999999411882    steps: 8535     evaluation reward: -18.714285714285715\n",
      "episode: 28   score: -13.0   memory length: 185459   epsilon: 0.009999999999411882    steps: 10248     evaluation reward: -18.517241379310345\n",
      "episode: 29   score: -16.0   memory length: 196174   epsilon: 0.009999999999411882    steps: 10715     evaluation reward: -18.433333333333334\n",
      "now time :  2018-12-20 01:55:38.249993\n",
      "episode: 30   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8778     evaluation reward: -18.419354838709676\n",
      "episode: 31   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10152     evaluation reward: -18.3125\n",
      "episode: 32   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9565     evaluation reward: -18.242424242424242\n",
      "episode: 33   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9480     evaluation reward: -18.147058823529413\n",
      "episode: 34   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10928     evaluation reward: -18.057142857142857\n",
      "now time :  2018-12-20 02:09:16.893956\n",
      "episode: 35   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 11146     evaluation reward: -17.97222222222222\n",
      "episode: 36   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9962     evaluation reward: -17.945945945945947\n",
      "episode: 37   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6929     evaluation reward: -17.973684210526315\n",
      "episode: 38   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9105     evaluation reward: -17.897435897435898\n",
      "episode: 39   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9499     evaluation reward: -17.85\n",
      "now time :  2018-12-20 02:22:57.045930\n",
      "episode: 40   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8389     evaluation reward: -17.853658536585368\n",
      "episode: 41   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7476     evaluation reward: -17.857142857142858\n",
      "episode: 42   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10967     evaluation reward: -17.813953488372093\n",
      "episode: 43   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7428     evaluation reward: -17.795454545454547\n",
      "episode: 44   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7872     evaluation reward: -17.8\n",
      "episode: 45   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10563     evaluation reward: -17.73913043478261\n",
      "now time :  2018-12-20 02:36:47.568926\n",
      "episode: 46   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9901     evaluation reward: -17.74468085106383\n",
      "episode: 47   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10738     evaluation reward: -17.6875\n",
      "episode: 48   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9395     evaluation reward: -17.653061224489797\n",
      "episode: 49   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10804     evaluation reward: -17.64\n",
      "episode: 50   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10417     evaluation reward: -17.647058823529413\n",
      "now time :  2018-12-20 02:50:36.576339\n",
      "episode: 51   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8277     evaluation reward: -17.653846153846153\n",
      "episode: 52   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8010     evaluation reward: -17.660377358490567\n",
      "episode: 53   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10083     evaluation reward: -17.62962962962963\n",
      "episode: 54   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8882     evaluation reward: -17.618181818181817\n",
      "episode: 55   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10474     evaluation reward: -17.571428571428573\n",
      "episode: 56   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8341     evaluation reward: -17.57894736842105\n",
      "now time :  2018-12-20 03:04:26.653971\n",
      "episode: 57   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8031     evaluation reward: -17.586206896551722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 58   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9659     evaluation reward: -17.559322033898304\n",
      "episode: 59   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8145     evaluation reward: -17.566666666666666\n",
      "episode: 60   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8641     evaluation reward: -17.524590163934427\n",
      "episode: 61   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9892     evaluation reward: -17.532258064516128\n",
      "now time :  2018-12-20 03:18:16.636898\n",
      "episode: 62   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9891     evaluation reward: -17.50793650793651\n",
      "episode: 63   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 12893     evaluation reward: -17.390625\n",
      "episode: 64   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9059     evaluation reward: -17.4\n",
      "episode: 65   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9198     evaluation reward: -17.393939393939394\n",
      "episode: 66   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8630     evaluation reward: -17.402985074626866\n",
      "now time :  2018-12-20 03:32:07.978356\n",
      "episode: 67   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7375     evaluation reward: -17.41176470588235\n",
      "episode: 68   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8957     evaluation reward: -17.420289855072465\n",
      "episode: 69   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10159     evaluation reward: -17.385714285714286\n",
      "episode: 70   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9285     evaluation reward: -17.3943661971831\n",
      "episode: 71   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8676     evaluation reward: -17.416666666666668\n",
      "episode: 72   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8639     evaluation reward: -17.424657534246574\n",
      "now time :  2018-12-20 03:45:58.265715\n",
      "episode: 73   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6740     evaluation reward: -17.41891891891892\n",
      "episode: 74   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9161     evaluation reward: -17.426666666666666\n",
      "episode: 75   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8924     evaluation reward: -17.38157894736842\n",
      "episode: 76   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9550     evaluation reward: -17.376623376623378\n",
      "episode: 77   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6311     evaluation reward: -17.384615384615383\n",
      "episode: 78   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8223     evaluation reward: -17.39240506329114\n",
      "now time :  2018-12-20 03:59:47.424881\n",
      "episode: 79   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8471     evaluation reward: -17.4\n",
      "episode: 80   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8509     evaluation reward: -17.40740740740741\n",
      "episode: 81   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9865     evaluation reward: -17.390243902439025\n",
      "episode: 82   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9343     evaluation reward: -17.397590361445783\n",
      "episode: 83   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7865     evaluation reward: -17.404761904761905\n",
      "now time :  2018-12-20 04:13:40.016454\n",
      "episode: 84   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10384     evaluation reward: -17.376470588235293\n",
      "episode: 85   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4969     evaluation reward: -17.3953488372093\n",
      "episode: 86   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 14492     evaluation reward: -17.32183908045977\n",
      "episode: 87   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 13293     evaluation reward: -17.261363636363637\n",
      "episode: 88   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 11813     evaluation reward: -17.247191011235955\n",
      "now time :  2018-12-20 04:27:31.505341\n",
      "episode: 89   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10194     evaluation reward: -17.255555555555556\n",
      "episode: 90   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5805     evaluation reward: -17.263736263736263\n",
      "episode: 91   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 10718     evaluation reward: -17.23913043478261\n",
      "episode: 92   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9853     evaluation reward: -17.247311827956988\n",
      "episode: 93   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7478     evaluation reward: -17.25531914893617\n",
      "episode: 94   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6293     evaluation reward: -17.263157894736842\n",
      "now time :  2018-12-20 04:41:22.962137\n",
      "episode: 95   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 7304     evaluation reward: -17.270833333333332\n",
      "episode: 96   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 8629     evaluation reward: -17.278350515463917\n",
      "episode: 97   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 9106     evaluation reward: -17.285714285714285\n",
      "episode: 98   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 12209     evaluation reward: -17.232323232323232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2b8afcf28c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Update the target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUpdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/downloads/github/deepRL/agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/downloads/github/deepRL/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    320\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_buffers'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        #if render_breakout:\n",
    "        #    env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/ec1_breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > (-15):\n",
    "                torch.save(agent.policy_net, \"./save_model/ec1_pongv4\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/ec1_pongv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
