{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from agent import *\n",
    "from model import *\n",
    "from config import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialise our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lifes(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3\n",
    "rewards, episodes = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. \n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -21.0   memory length: 1140   epsilon: 1.0    steps: 1140     evaluation reward: -21.0\n",
      "episode: 1   score: -21.0   memory length: 2349   epsilon: 1.0    steps: 1209     evaluation reward: -21.0\n",
      "episode: 2   score: -21.0   memory length: 3533   epsilon: 1.0    steps: 1184     evaluation reward: -21.0\n",
      "episode: 3   score: -21.0   memory length: 4548   epsilon: 1.0    steps: 1015     evaluation reward: -21.0\n",
      "episode: 4   score: -20.0   memory length: 5758   epsilon: 1.0    steps: 1210     evaluation reward: -20.8\n",
      "episode: 5   score: -21.0   memory length: 6857   epsilon: 1.0    steps: 1099     evaluation reward: -20.833333333333332\n",
      "episode: 6   score: -20.0   memory length: 8377   epsilon: 1.0    steps: 1520     evaluation reward: -20.714285714285715\n",
      "episode: 7   score: -20.0   memory length: 9666   epsilon: 1.0    steps: 1289     evaluation reward: -20.625\n",
      "episode: 8   score: -21.0   memory length: 10848   epsilon: 0.991594899999994    steps: 1182     evaluation reward: -20.666666666666668\n",
      "episode: 9   score: -21.0   memory length: 12024   epsilon: 0.9799524999999856    steps: 1176     evaluation reward: -20.7\n",
      "episode: 10   score: -20.0   memory length: 13226   epsilon: 0.968052699999977    steps: 1202     evaluation reward: -20.636363636363637\n",
      "episode: 11   score: -16.0   memory length: 15132   epsilon: 0.9491832999999634    steps: 1906     evaluation reward: -20.25\n",
      "episode: 12   score: -19.0   memory length: 16459   epsilon: 0.936045999999954    steps: 1327     evaluation reward: -20.153846153846153\n",
      "episode: 13   score: -20.0   memory length: 17833   epsilon: 0.9224433999999442    steps: 1374     evaluation reward: -20.142857142857142\n",
      "episode: 14   score: -20.0   memory length: 19444   epsilon: 0.9064944999999327    steps: 1611     evaluation reward: -20.133333333333333\n",
      "episode: 15   score: -20.0   memory length: 20817   epsilon: 0.8929017999999229    steps: 1373     evaluation reward: -20.125\n",
      "episode: 16   score: -20.0   memory length: 22187   epsilon: 0.8793387999999132    steps: 1370     evaluation reward: -20.11764705882353\n",
      "episode: 17   score: -21.0   memory length: 23404   epsilon: 0.8672904999999045    steps: 1217     evaluation reward: -20.166666666666668\n",
      "episode: 18   score: -19.0   memory length: 24700   epsilon: 0.8544600999998953    steps: 1296     evaluation reward: -20.105263157894736\n",
      "episode: 19   score: -21.0   memory length: 25957   epsilon: 0.8420157999998863    steps: 1257     evaluation reward: -20.15\n",
      "episode: 20   score: -21.0   memory length: 27078   epsilon: 0.8309178999998783    steps: 1121     evaluation reward: -20.19047619047619\n",
      "episode: 21   score: -21.0   memory length: 28256   epsilon: 0.8192556999998699    steps: 1178     evaluation reward: -20.227272727272727\n",
      "episode: 22   score: -19.0   memory length: 29808   epsilon: 0.8038908999998589    steps: 1552     evaluation reward: -20.17391304347826\n",
      "episode: 23   score: -21.0   memory length: 31018   epsilon: 0.7919118999998502    steps: 1210     evaluation reward: -20.208333333333332\n",
      "episode: 24   score: -21.0   memory length: 32408   epsilon: 0.7781508999998403    steps: 1390     evaluation reward: -20.24\n",
      "episode: 25   score: -18.0   memory length: 34169   epsilon: 0.7607169999998278    steps: 1761     evaluation reward: -20.153846153846153\n",
      "episode: 26   score: -20.0   memory length: 36086   epsilon: 0.7417386999998141    steps: 1917     evaluation reward: -20.14814814814815\n",
      "episode: 27   score: -19.0   memory length: 37852   epsilon: 0.7242552999998015    steps: 1766     evaluation reward: -20.107142857142858\n",
      "episode: 28   score: -19.0   memory length: 39260   epsilon: 0.7103160999997915    steps: 1408     evaluation reward: -20.06896551724138\n",
      "episode: 29   score: -21.0   memory length: 41091   epsilon: 0.6921891999997785    steps: 1831     evaluation reward: -20.1\n",
      "episode: 30   score: -21.0   memory length: 42405   epsilon: 0.6791805999997691    steps: 1314     evaluation reward: -20.129032258064516\n",
      "episode: 31   score: -19.0   memory length: 44382   epsilon: 0.659608299999755    steps: 1977     evaluation reward: -20.09375\n",
      "episode: 32   score: -18.0   memory length: 45937   epsilon: 0.6442137999997439    steps: 1555     evaluation reward: -20.03030303030303\n",
      "episode: 33   score: -19.0   memory length: 48352   epsilon: 0.6203052999997267    steps: 2415     evaluation reward: -20.0\n",
      "now time :  2018-12-19 17:03:33.040708\n",
      "episode: 34   score: -18.0   memory length: 50221   epsilon: 0.6018021999997134    steps: 1869     evaluation reward: -19.942857142857143\n",
      "episode: 35   score: -17.0   memory length: 52467   epsilon: 0.5795667999996974    steps: 2246     evaluation reward: -19.86111111111111\n",
      "episode: 36   score: -18.0   memory length: 54900   epsilon: 0.5554800999996801    steps: 2433     evaluation reward: -19.81081081081081\n",
      "episode: 37   score: -16.0   memory length: 57978   epsilon: 0.5250078999996581    steps: 3078     evaluation reward: -19.710526315789473\n",
      "episode: 38   score: -19.0   memory length: 60075   epsilon: 0.5042475999996432    steps: 2097     evaluation reward: -19.692307692307693\n",
      "episode: 39   score: -20.0   memory length: 61945   epsilon: 0.48573459999962987    steps: 1870     evaluation reward: -19.7\n",
      "episode: 40   score: -15.0   memory length: 65164   epsilon: 0.45386649999960693    steps: 3219     evaluation reward: -19.585365853658537\n",
      "episode: 41   score: -18.0   memory length: 67603   epsilon: 0.42972039999958955    steps: 2439     evaluation reward: -19.547619047619047\n",
      "episode: 42   score: -17.0   memory length: 70333   epsilon: 0.4026933999995701    steps: 2730     evaluation reward: -19.488372093023255\n",
      "episode: 43   score: -19.0   memory length: 73059   epsilon: 0.3757059999995507    steps: 2726     evaluation reward: -19.477272727272727\n",
      "episode: 44   score: -20.0   memory length: 75812   epsilon: 0.34845129999953106    steps: 2753     evaluation reward: -19.488888888888887\n",
      "episode: 45   score: -17.0   memory length: 78402   epsilon: 0.3228102999995126    steps: 2590     evaluation reward: -19.434782608695652\n",
      "episode: 46   score: -13.0   memory length: 82148   epsilon: 0.2857248999994859    steps: 3746     evaluation reward: -19.29787234042553\n",
      "episode: 47   score: -16.0   memory length: 85818   epsilon: 0.24939189999945977    steps: 3670     evaluation reward: -19.229166666666668\n",
      "episode: 48   score: -16.0   memory length: 89132   epsilon: 0.21658329999943615    steps: 3314     evaluation reward: -19.163265306122447\n",
      "episode: 49   score: -15.0   memory length: 92458   epsilon: 0.18365589999941245    steps: 3326     evaluation reward: -19.08\n",
      "episode: 50   score: -17.0   memory length: 96295   epsilon: 0.1456695999993851    steps: 3837     evaluation reward: -19.03921568627451\n",
      "now time :  2018-12-19 17:16:08.906660\n",
      "episode: 51   score: -12.0   memory length: 100951   epsilon: 0.09957519999938759    steps: 4656     evaluation reward: -18.903846153846153\n",
      "episode: 52   score: -18.0   memory length: 104484   epsilon: 0.06459849999941145    steps: 3533     evaluation reward: -18.88679245283019\n",
      "episode: 53   score: -16.0   memory length: 108223   epsilon: 0.027582399999412213    steps: 3739     evaluation reward: -18.833333333333332\n",
      "episode: 54   score: -11.0   memory length: 112678   epsilon: 0.009999999999411882    steps: 4455     evaluation reward: -18.69090909090909\n",
      "episode: 55   score: -17.0   memory length: 117370   epsilon: 0.009999999999411882    steps: 4692     evaluation reward: -18.660714285714285\n",
      "episode: 56   score: -11.0   memory length: 122826   epsilon: 0.009999999999411882    steps: 5456     evaluation reward: -18.526315789473685\n",
      "episode: 57   score: -15.0   memory length: 127611   epsilon: 0.009999999999411882    steps: 4785     evaluation reward: -18.46551724137931\n",
      "episode: 58   score: -8.0   memory length: 133687   epsilon: 0.009999999999411882    steps: 6076     evaluation reward: -18.28813559322034\n",
      "episode: 59   score: -13.0   memory length: 138274   epsilon: 0.009999999999411882    steps: 4587     evaluation reward: -18.2\n",
      "episode: 60   score: -11.0   memory length: 142257   epsilon: 0.009999999999411882    steps: 3983     evaluation reward: -18.081967213114755\n",
      "episode: 61   score: -15.0   memory length: 145659   epsilon: 0.009999999999411882    steps: 3402     evaluation reward: -18.032258064516128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 62   score: -14.0   memory length: 149555   epsilon: 0.009999999999411882    steps: 3896     evaluation reward: -17.96825396825397\n",
      "now time :  2018-12-19 17:29:35.348364\n",
      "episode: 63   score: -14.0   memory length: 154022   epsilon: 0.009999999999411882    steps: 4467     evaluation reward: -17.90625\n",
      "episode: 64   score: -10.0   memory length: 158914   epsilon: 0.009999999999411882    steps: 4892     evaluation reward: -17.784615384615385\n",
      "episode: 65   score: -13.0   memory length: 163602   epsilon: 0.009999999999411882    steps: 4688     evaluation reward: -17.71212121212121\n",
      "episode: 66   score: -8.0   memory length: 168852   epsilon: 0.009999999999411882    steps: 5250     evaluation reward: -17.567164179104477\n",
      "episode: 67   score: -14.0   memory length: 173838   epsilon: 0.009999999999411882    steps: 4986     evaluation reward: -17.514705882352942\n",
      "episode: 68   score: -17.0   memory length: 178110   epsilon: 0.009999999999411882    steps: 4272     evaluation reward: -17.507246376811594\n",
      "episode: 69   score: -13.0   memory length: 182761   epsilon: 0.009999999999411882    steps: 4651     evaluation reward: -17.442857142857143\n",
      "episode: 70   score: -21.0   memory length: 185832   epsilon: 0.009999999999411882    steps: 3071     evaluation reward: -17.492957746478872\n",
      "episode: 71   score: -13.0   memory length: 190771   epsilon: 0.009999999999411882    steps: 4939     evaluation reward: -17.430555555555557\n",
      "episode: 72   score: -18.0   memory length: 194485   epsilon: 0.009999999999411882    steps: 3714     evaluation reward: -17.438356164383563\n",
      "episode: 73   score: -16.0   memory length: 199411   epsilon: 0.009999999999411882    steps: 4926     evaluation reward: -17.41891891891892\n",
      "now time :  2018-12-19 17:43:44.025887\n",
      "episode: 74   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4232     evaluation reward: -17.4\n",
      "episode: 75   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4007     evaluation reward: -17.355263157894736\n",
      "episode: 76   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4152     evaluation reward: -17.2987012987013\n",
      "episode: 77   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4432     evaluation reward: -17.28205128205128\n",
      "episode: 78   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4367     evaluation reward: -17.21518987341772\n",
      "episode: 79   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4108     evaluation reward: -17.15\n",
      "episode: 80   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3512     evaluation reward: -17.14814814814815\n",
      "episode: 81   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4602     evaluation reward: -17.121951219512194\n",
      "episode: 82   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3942     evaluation reward: -17.156626506024097\n",
      "episode: 83   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5560     evaluation reward: -17.095238095238095\n",
      "episode: 84   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4092     evaluation reward: -17.094117647058823\n",
      "now time :  2018-12-19 17:58:45.611420\n",
      "episode: 85   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4603     evaluation reward: -17.058139534883722\n",
      "episode: 86   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3644     evaluation reward: -17.03448275862069\n",
      "episode: 87   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4252     evaluation reward: -17.0\n",
      "episode: 88   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3879     evaluation reward: -16.98876404494382\n",
      "episode: 89   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5240     evaluation reward: -16.91111111111111\n",
      "episode: 90   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4258     evaluation reward: -16.9010989010989\n",
      "episode: 91   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3929     evaluation reward: -16.847826086956523\n",
      "episode: 92   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5540     evaluation reward: -16.774193548387096\n",
      "episode: 93   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4338     evaluation reward: -16.74468085106383\n",
      "episode: 94   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3703     evaluation reward: -16.768421052631577\n",
      "episode: 95   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4627     evaluation reward: -16.6875\n",
      "episode: 96   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4626     evaluation reward: -16.649484536082475\n",
      "now time :  2018-12-19 18:14:11.741603\n",
      "episode: 97   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4448     evaluation reward: -16.612244897959183\n",
      "episode: 98   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3867     evaluation reward: -16.595959595959595\n",
      "episode: 99   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3732     evaluation reward: -16.57\n",
      "episode: 100   score: -8.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5860     evaluation reward: -16.44\n",
      "episode: 101   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5201     evaluation reward: -16.36\n",
      "episode: 102   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3854     evaluation reward: -16.33\n",
      "episode: 103   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3855     evaluation reward: -16.28\n",
      "episode: 104   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4026     evaluation reward: -16.23\n",
      "episode: 105   score: -8.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4864     evaluation reward: -16.1\n",
      "episode: 106   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4270     evaluation reward: -16.0\n",
      "episode: 107   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4113     evaluation reward: -15.98\n",
      "now time :  2018-12-19 18:29:44.511001\n",
      "episode: 108   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4443     evaluation reward: -15.91\n",
      "episode: 109   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4088     evaluation reward: -15.9\n",
      "episode: 110   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4021     evaluation reward: -15.83\n",
      "episode: 111   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4755     evaluation reward: -15.79\n",
      "episode: 112   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4571     evaluation reward: -15.73\n",
      "episode: 113   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4463     evaluation reward: -15.71\n",
      "episode: 114   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2549     evaluation reward: -15.69\n",
      "episode: 115   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4377     evaluation reward: -15.62\n",
      "episode: 116   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5000     evaluation reward: -15.52\n",
      "episode: 117   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4358     evaluation reward: -15.47\n",
      "episode: 118   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3257     evaluation reward: -15.44\n",
      "episode: 119   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3730     evaluation reward: -15.42\n",
      "now time :  2018-12-19 18:45:15.804151\n",
      "episode: 120   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4531     evaluation reward: -15.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 121   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3429     evaluation reward: -15.33\n",
      "episode: 122   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3985     evaluation reward: -15.27\n",
      "episode: 123   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3346     evaluation reward: -15.22\n",
      "episode: 124   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5106     evaluation reward: -15.13\n",
      "episode: 125   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5066     evaluation reward: -15.06\n",
      "episode: 126   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4062     evaluation reward: -14.99\n",
      "episode: 127   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5331     evaluation reward: -14.92\n",
      "episode: 128   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4204     evaluation reward: -14.84\n",
      "episode: 129   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4676     evaluation reward: -14.74\n",
      "episode: 130   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4334     evaluation reward: -14.71\n",
      "episode: 131   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4422     evaluation reward: -14.63\n",
      "now time :  2018-12-19 19:00:52.229579\n",
      "episode: 132   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4090     evaluation reward: -14.63\n",
      "episode: 133   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5764     evaluation reward: -14.54\n",
      "episode: 134   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2615     evaluation reward: -14.56\n",
      "episode: 135   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4210     evaluation reward: -14.54\n",
      "episode: 136   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2304     evaluation reward: -14.55\n",
      "episode: 137   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5068     evaluation reward: -14.52\n",
      "episode: 138   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3105     evaluation reward: -14.5\n",
      "episode: 139   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3850     evaluation reward: -14.44\n",
      "episode: 140   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3146     evaluation reward: -14.48\n",
      "episode: 141   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3946     evaluation reward: -14.46\n",
      "episode: 142   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5488     evaluation reward: -14.42\n",
      "episode: 143   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4183     evaluation reward: -14.37\n",
      "now time :  2018-12-19 19:16:32.132803\n",
      "episode: 144   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4358     evaluation reward: -14.32\n",
      "episode: 145   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5323     evaluation reward: -14.24\n",
      "episode: 146   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4680     evaluation reward: -14.27\n",
      "episode: 147   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3958     evaluation reward: -14.3\n",
      "episode: 148   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4259     evaluation reward: -14.29\n",
      "episode: 149   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4097     evaluation reward: -14.31\n",
      "episode: 150   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3218     evaluation reward: -14.34\n",
      "episode: 151   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4869     evaluation reward: -14.38\n",
      "episode: 152   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4841     evaluation reward: -14.37\n",
      "episode: 153   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4738     evaluation reward: -14.33\n",
      "episode: 154   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4108     evaluation reward: -14.38\n",
      "now time :  2018-12-19 19:32:09.377270\n",
      "episode: 155   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4670     evaluation reward: -14.34\n",
      "episode: 156   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4300     evaluation reward: -14.38\n",
      "episode: 157   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4451     evaluation reward: -14.38\n",
      "episode: 158   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4958     evaluation reward: -14.43\n",
      "episode: 159   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3440     evaluation reward: -14.49\n",
      "episode: 160   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4213     evaluation reward: -14.5\n",
      "episode: 161   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4458     evaluation reward: -14.5\n",
      "episode: 162   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4212     evaluation reward: -14.51\n",
      "episode: 163   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4371     evaluation reward: -14.52\n",
      "episode: 164   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4358     evaluation reward: -14.56\n",
      "episode: 165   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3343     evaluation reward: -14.59\n",
      "episode: 166   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4189     evaluation reward: -14.66\n",
      "now time :  2018-12-19 19:47:46.441061\n",
      "episode: 167   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5799     evaluation reward: -14.66\n",
      "episode: 168   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3447     evaluation reward: -14.68\n",
      "episode: 169   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4829     evaluation reward: -14.65\n",
      "episode: 170   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5420     evaluation reward: -14.58\n",
      "episode: 171   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5337     evaluation reward: -14.57\n",
      "episode: 172   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3262     evaluation reward: -14.53\n",
      "episode: 173   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3032     evaluation reward: -14.5\n",
      "episode: 174   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4253     evaluation reward: -14.5\n",
      "episode: 175   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3599     evaluation reward: -14.51\n",
      "episode: 176   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3836     evaluation reward: -14.54\n",
      "episode: 177   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2929     evaluation reward: -14.56\n",
      "episode: 178   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4401     evaluation reward: -14.59\n",
      "now time :  2018-12-19 20:03:19.514132\n",
      "episode: 179   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5290     evaluation reward: -14.56\n",
      "episode: 180   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3880     evaluation reward: -14.55\n",
      "episode: 181   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3323     evaluation reward: -14.59\n",
      "episode: 182   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4634     evaluation reward: -14.56\n",
      "episode: 183   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3001     evaluation reward: -14.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 184   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5427     evaluation reward: -14.62\n",
      "episode: 185   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3964     evaluation reward: -14.66\n",
      "episode: 186   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3563     evaluation reward: -14.66\n",
      "episode: 188   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5231     evaluation reward: -14.67\n",
      "episode: 189   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4123     evaluation reward: -14.75\n",
      "episode: 190   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4048     evaluation reward: -14.77\n",
      "now time :  2018-12-19 20:18:52.400744\n",
      "episode: 191   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4882     evaluation reward: -14.78\n",
      "episode: 192   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4302     evaluation reward: -14.84\n",
      "episode: 193   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3745     evaluation reward: -14.85\n",
      "episode: 194   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3950     evaluation reward: -14.8\n",
      "episode: 195   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4303     evaluation reward: -14.85\n",
      "episode: 196   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3627     evaluation reward: -14.9\n",
      "episode: 197   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3035     evaluation reward: -14.94\n",
      "episode: 198   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4199     evaluation reward: -14.94\n",
      "episode: 199   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4296     evaluation reward: -14.99\n",
      "episode: 200   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4919     evaluation reward: -15.04\n",
      "episode: 201   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4533     evaluation reward: -15.05\n",
      "episode: 202   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5047     evaluation reward: -15.01\n",
      "now time :  2018-12-19 20:34:24.481600\n",
      "episode: 203   score: -7.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5804     evaluation reward: -14.92\n",
      "episode: 204   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4182     evaluation reward: -14.92\n",
      "episode: 205   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3503     evaluation reward: -15.01\n",
      "episode: 206   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4858     evaluation reward: -15.0\n",
      "episode: 207   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3211     evaluation reward: -14.98\n",
      "episode: 208   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6022     evaluation reward: -14.98\n",
      "episode: 209   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4428     evaluation reward: -14.94\n",
      "episode: 210   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3865     evaluation reward: -14.98\n",
      "episode: 211   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3426     evaluation reward: -14.99\n",
      "episode: 212   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4559     evaluation reward: -15.02\n",
      "episode: 213   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3994     evaluation reward: -15.0\n",
      "now time :  2018-12-19 20:49:58.196573\n",
      "episode: 214   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4671     evaluation reward: -14.98\n",
      "episode: 215   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3681     evaluation reward: -15.01\n",
      "episode: 216   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3649     evaluation reward: -15.08\n",
      "episode: 217   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3163     evaluation reward: -15.1\n",
      "episode: 218   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2813     evaluation reward: -15.14\n",
      "episode: 219   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5944     evaluation reward: -15.05\n",
      "episode: 220   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4156     evaluation reward: -15.06\n",
      "episode: 221   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5394     evaluation reward: -14.99\n",
      "episode: 222   score: -4.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5509     evaluation reward: -14.9\n",
      "episode: 223   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3833     evaluation reward: -14.89\n",
      "episode: 224   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3489     evaluation reward: -14.93\n",
      "episode: 225   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4191     evaluation reward: -14.98\n",
      "episode: 226   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3945     evaluation reward: -15.03\n",
      "now time :  2018-12-19 21:05:35.364404\n",
      "episode: 227   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3572     evaluation reward: -15.07\n",
      "episode: 228   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3740     evaluation reward: -15.12\n",
      "episode: 229   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4715     evaluation reward: -15.14\n",
      "episode: 230   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4779     evaluation reward: -15.12\n",
      "episode: 231   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4237     evaluation reward: -15.14\n",
      "episode: 232   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4506     evaluation reward: -15.08\n",
      "episode: 233   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5102     evaluation reward: -15.12\n",
      "episode: 234   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3996     evaluation reward: -15.12\n",
      "episode: 235   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5448     evaluation reward: -15.06\n",
      "episode: 236   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4592     evaluation reward: -14.99\n",
      "episode: 237   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3742     evaluation reward: -15.02\n",
      "now time :  2018-12-19 21:21:10.663363\n",
      "episode: 238   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4366     evaluation reward: -15.01\n",
      "episode: 240   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4189     evaluation reward: -15.04\n",
      "episode: 241   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4735     evaluation reward: -15.03\n",
      "episode: 242   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5455     evaluation reward: -15.0\n",
      "episode: 243   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3923     evaluation reward: -15.04\n",
      "episode: 244   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5055     evaluation reward: -15.0\n",
      "episode: 245   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3966     evaluation reward: -15.07\n",
      "episode: 246   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3358     evaluation reward: -15.08\n",
      "episode: 247   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4072     evaluation reward: -15.04\n",
      "episode: 248   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4704     evaluation reward: -15.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now time :  2018-12-19 21:36:43.303656\n",
      "episode: 249   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3866     evaluation reward: -15.03\n",
      "episode: 250   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4763     evaluation reward: -14.98\n",
      "episode: 251   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4474     evaluation reward: -15.01\n",
      "episode: 252   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4002     evaluation reward: -14.99\n",
      "episode: 253   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4462     evaluation reward: -15.01\n",
      "episode: 254   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5263     evaluation reward: -14.98\n",
      "episode: 255   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3886     evaluation reward: -15.03\n",
      "episode: 256   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4561     evaluation reward: -15.03\n",
      "episode: 257   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4623     evaluation reward: -15.01\n",
      "episode: 258   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3356     evaluation reward: -15.05\n",
      "episode: 259   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5131     evaluation reward: -15.0\n",
      "episode: 260   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4120     evaluation reward: -15.02\n",
      "now time :  2018-12-19 21:52:15.927947\n",
      "episode: 261   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4269     evaluation reward: -15.03\n",
      "episode: 262   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3881     evaluation reward: -15.02\n",
      "episode: 263   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4287     evaluation reward: -15.01\n",
      "episode: 264   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4867     evaluation reward: -14.99\n",
      "episode: 265   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3773     evaluation reward: -14.99\n",
      "episode: 266   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3256     evaluation reward: -15.0\n",
      "episode: 267   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4594     evaluation reward: -15.03\n",
      "episode: 268   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3957     evaluation reward: -15.01\n",
      "episode: 269   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3397     evaluation reward: -15.1\n",
      "episode: 270   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5058     evaluation reward: -15.11\n",
      "episode: 271   score: -7.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5228     evaluation reward: -15.06\n",
      "episode: 272   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3611     evaluation reward: -15.07\n",
      "now time :  2018-12-19 22:07:47.687003\n",
      "episode: 273   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4243     evaluation reward: -15.11\n",
      "episode: 274   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5579     evaluation reward: -15.06\n",
      "episode: 275   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4343     evaluation reward: -15.02\n",
      "episode: 276   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4775     evaluation reward: -15.0\n",
      "episode: 277   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3389     evaluation reward: -15.0\n",
      "episode: 278   score: -8.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5724     evaluation reward: -14.93\n",
      "episode: 279   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3968     evaluation reward: -15.02\n",
      "episode: 280   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5129     evaluation reward: -14.99\n",
      "episode: 281   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5542     evaluation reward: -14.92\n",
      "episode: 282   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4347     evaluation reward: -14.92\n",
      "now time :  2018-12-19 22:23:16.137543\n",
      "episode: 283   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4698     evaluation reward: -14.86\n",
      "episode: 284   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4336     evaluation reward: -14.87\n",
      "episode: 285   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3487     evaluation reward: -14.86\n",
      "episode: 286   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2766     evaluation reward: -14.88\n",
      "episode: 287   score: -9.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4642     evaluation reward: -14.81\n",
      "episode: 288   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4424     evaluation reward: -14.79\n",
      "episode: 289   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4355     evaluation reward: -14.76\n",
      "episode: 290   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3668     evaluation reward: -14.74\n",
      "episode: 291   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4404     evaluation reward: -14.74\n",
      "episode: 292   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5640     evaluation reward: -14.68\n",
      "episode: 293   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5028     evaluation reward: -14.64\n",
      "episode: 294   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3916     evaluation reward: -14.66\n",
      "now time :  2018-12-19 22:38:43.379011\n",
      "episode: 295   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3093     evaluation reward: -14.71\n",
      "episode: 296   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4404     evaluation reward: -14.68\n",
      "episode: 297   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4499     evaluation reward: -14.64\n",
      "episode: 298   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4372     evaluation reward: -14.67\n",
      "episode: 299   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5043     evaluation reward: -14.64\n",
      "episode: 300   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4238     evaluation reward: -14.66\n",
      "episode: 301   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4714     evaluation reward: -14.66\n",
      "episode: 302   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4912     evaluation reward: -14.64\n",
      "episode: 303   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2826     evaluation reward: -14.76\n",
      "episode: 304   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4949     evaluation reward: -14.75\n",
      "episode: 305   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4549     evaluation reward: -14.72\n",
      "episode: 306   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2775     evaluation reward: -14.82\n",
      "now time :  2018-12-19 22:54:11.259138\n",
      "episode: 307   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4451     evaluation reward: -14.79\n",
      "episode: 308   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3724     evaluation reward: -14.79\n",
      "episode: 309   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4324     evaluation reward: -14.77\n",
      "episode: 310   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4863     evaluation reward: -14.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 311   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4510     evaluation reward: -14.78\n",
      "episode: 312   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3225     evaluation reward: -14.79\n",
      "episode: 313   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3495     evaluation reward: -14.78\n",
      "episode: 314   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4982     evaluation reward: -14.74\n",
      "episode: 315   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4647     evaluation reward: -14.76\n",
      "episode: 316   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4023     evaluation reward: -14.79\n",
      "episode: 317   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3191     evaluation reward: -14.8\n",
      "episode: 318   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4435     evaluation reward: -14.73\n",
      "now time :  2018-12-19 23:09:40.588253\n",
      "episode: 319   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3987     evaluation reward: -14.76\n",
      "episode: 320   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4810     evaluation reward: -14.71\n",
      "episode: 321   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 6080     evaluation reward: -14.72\n",
      "episode: 322   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3661     evaluation reward: -14.84\n",
      "episode: 323   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5367     evaluation reward: -14.79\n",
      "episode: 324   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4643     evaluation reward: -14.77\n",
      "episode: 325   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5010     evaluation reward: -14.74\n",
      "episode: 326   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2911     evaluation reward: -14.75\n",
      "episode: 327   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3687     evaluation reward: -14.78\n",
      "episode: 328   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4533     evaluation reward: -14.75\n",
      "episode: 329   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5709     evaluation reward: -14.72\n",
      "episode: 331   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4347     evaluation reward: -14.78\n",
      "episode: 332   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5419     evaluation reward: -14.77\n",
      "episode: 333   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4056     evaluation reward: -14.76\n",
      "episode: 334   score: -10.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5392     evaluation reward: -14.66\n",
      "episode: 335   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3707     evaluation reward: -14.72\n",
      "episode: 336   score: -20.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3198     evaluation reward: -14.8\n",
      "episode: 337   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3484     evaluation reward: -14.79\n",
      "episode: 338   score: -12.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4769     evaluation reward: -14.75\n",
      "episode: 339   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4672     evaluation reward: -14.72\n",
      "episode: 340   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4613     evaluation reward: -14.66\n",
      "episode: 341   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 2763     evaluation reward: -14.69\n",
      "now time :  2018-12-19 23:40:33.345329\n",
      "episode: 342   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5149     evaluation reward: -14.72\n",
      "episode: 343   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4523     evaluation reward: -14.67\n",
      "episode: 344   score: -16.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4568     evaluation reward: -14.72\n",
      "episode: 345   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3718     evaluation reward: -14.73\n",
      "episode: 346   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3674     evaluation reward: -14.7\n",
      "episode: 347   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3985     evaluation reward: -14.7\n",
      "episode: 348   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4430     evaluation reward: -14.66\n",
      "episode: 349   score: -19.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3797     evaluation reward: -14.72\n",
      "episode: 350   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3583     evaluation reward: -14.7\n",
      "episode: 351   score: -17.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4742     evaluation reward: -14.68\n",
      "episode: 352   score: -14.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4695     evaluation reward: -14.67\n",
      "episode: 353   score: -13.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4029     evaluation reward: -14.66\n",
      "now time :  2018-12-19 23:55:55.232214\n",
      "episode: 354   score: -11.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 4959     evaluation reward: -14.64\n",
      "episode: 355   score: -18.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 3802     evaluation reward: -14.64\n",
      "episode: 356   score: -15.0   memory length: 200000   epsilon: 0.009999999999411882    steps: 5065     evaluation reward: -14.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e3d62bd38ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Update the target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUpdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/downloads/github/deepRL/agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    d = False\n",
    "    state = env.reset()\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "        #if render_breakout:\n",
    "        #    env.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "\n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['ale.lives'])\n",
    "\n",
    "        life = info['ale.lives']\n",
    "        r = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network\n",
    "            if(frame % Update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "\n",
    "        if frame % 50000 == 0:\n",
    "            print('now time : ', datetime.now())\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.savefig(\"./save_graph/ec1_breakout_dqn.png\")\n",
    "\n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 10 episode is bigger than 400\n",
    "            # stop training\n",
    "            if np.mean(evaluation_reward) > 10000:\n",
    "                torch.save(agent.policy_net, \"./save_model/ec1_breakout_dqn2\")\n",
    "                sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/ec1_breakout_dqn2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
